import 'package:flutter/material.dart';
import 'dart:developer' as developer;
import 'package:shared_preferences/shared_preferences.dart';
import './producer_provider.dart';
import './consumer_provider.dart';
import '../ffi/kafka_ffi.dart';
import '../models/topic_model.dart';

class KafkaConnection {
  final String name;
  final String bootstrapServers;

  KafkaConnection({
    required this.name,
    required this.bootstrapServers,
  });

  String get servers => bootstrapServers;

  Map<String, dynamic> toMap() {
    return {
      'name': name,
      'bootstrapServers': bootstrapServers,
    };
  }

  factory KafkaConnection.fromMap(Map<String, dynamic> map) {
    return KafkaConnection(
      name: map['name'],
      bootstrapServers: map['bootstrapServers'],
    );
  }
}

class KafkaProvider extends ChangeNotifier {
  bool _isConnected = false;
  List<String> _topics = [
    'test-topic-1',
    'test-topic-2',
    'very-long-topic-name-that-should-be-truncated-test-1234567890',
    'kafka-test-topic-2025',
    'sample-topic-with-many-partitions',
    'new-topic-created-2025',
    'another-kafka-topic',
    'demo-topic-for-testing'
  ];
  List<KafkaConnection> _savedConnections = [];
  KafkaConnection? _currentConnection;
  KafkaClientHandle? _tempClient;

  // å­˜å‚¨ä¸»é¢˜è¯¦æƒ…çš„æ˜ å°„
  Map<String, TopicInfo> _topicDetails = {};
  Map<String, List<KafkaPartitionInfo>> _topicPartitions = {};
  Map<String, List<KafkaConfigParam>> _topicConfigs = {};
  Map<String, List<KafkaConsumerGroup>> _topicConsumerGroups = {};

  // å­Provider
  final ProducerProvider _producerProvider = ProducerProvider();
  final ConsumerProvider _consumerProvider = ConsumerProvider();

  // Getters
  List<KafkaConnection> get savedConnections => _savedConnections;
  KafkaConnection? get currentConnection => _currentConnection;
  bool get isConnected => _isConnected;
  List<String> get topics => _topics;
  ProducerProvider get producerProvider => _producerProvider;
  ConsumerProvider get consumerProvider => _consumerProvider;
  Map<String, TopicInfo> get topicDetails => _topicDetails;
  Map<String, List<KafkaPartitionInfo>> get topicPartitions => _topicPartitions;
  Map<String, List<KafkaConfigParam>> get topicConfigs => _topicConfigs;
  Map<String, List<KafkaConsumerGroup>> get topicConsumerGroups =>
      _topicConsumerGroups;

  Future<void> loadSavedConnections() async {
    try {
      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = prefs.getStringList('kafka_connections');

      if (connectionsJson != null) {
        _savedConnections = connectionsJson.map((json) {
          final parts = json.split('|||');
          return KafkaConnection.fromMap({
            'name': parts[0],
            'bootstrapServers': parts[1],
          });
        }).toList();
      }

      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to load saved connections: $e',
          stackTrace: stackTrace);
    }
  }

  Future<void> saveConnection(KafkaConnection connection) async {
    try {
      final existingIndex = _savedConnections.indexWhere(
        (c) => c.name == connection.name,
      );

      if (existingIndex != -1) {
        _savedConnections[existingIndex] = connection;
      } else {
        _savedConnections.add(connection);
      }

      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = _savedConnections
          .map((c) => '${c.name}|||${c.bootstrapServers}')
          .toList();

      await prefs.setStringList('kafka_connections', connectionsJson);
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to save connection: $e', stackTrace: stackTrace);
      throw Exception('Failed to save connection: $e');
    }
  }

  Future<void> deleteConnection(String connectionName) async {
    try {
      _savedConnections.removeWhere((c) => c.name == connectionName);

      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = _savedConnections
          .map((c) => '${c.name}|||${c.bootstrapServers}')
          .toList();

      await prefs.setStringList('kafka_connections', connectionsJson);
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to delete connection: $e', stackTrace: stackTrace);
      throw Exception('Failed to delete connection: $e');
    }
  }

  /// æµ‹è¯•ä¸Kafkaé›†ç¾¤çš„è¿æ¥
  Future<bool> testConnection(String bootstrapServers) async {
    try {
      developer.log('Testing connection to Kafka at $bootstrapServers via FFI');

      // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
      final tempClient = KafkaFFI.createProducer(bootstrapServers);

      // å°è¯•è·å–ä¸»é¢˜åˆ—è¡¨ï¼ŒéªŒè¯è¿æ¥æ˜¯å¦æˆåŠŸ
      final topics = KafkaFFI.getTopics(tempClient);

      // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
      KafkaFFI.closeClient(tempClient);

      developer
          .log('Connection test successful. Found ${topics.length} topics');
      return true;
    } catch (e, stackTrace) {
      developer.log('Connection test failed: $e', stackTrace: stackTrace);
      return false;
    }
  }

  Future<void> connect(String bootstrapServers,
      {String? connectionName}) async {
    try {
      KafkaConnection connection = KafkaConnection(
        name: connectionName ?? 'ä¸´æ—¶è¿æ¥',
        bootstrapServers: bootstrapServers,
      );

      developer.log(
          'Attempting to connect to Kafka at ${connection.bootstrapServers} via FFI');

      // ä½¿ç”¨ä¸´æ—¶å®¢æˆ·ç«¯è·å–ä¸»é¢˜åˆ—è¡¨
      _tempClient = KafkaFFI.createProducer(connection.bootstrapServers);
      await fetchTopics(connection);

      // è¿æ¥ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
      await _producerProvider.connect(connection.bootstrapServers);
      await _consumerProvider.connect(connection.bootstrapServers);

      _isConnected = true;
      _currentConnection = connection;

      // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      developer.log(
          'Successfully connected to Kafka at ${connection.bootstrapServers}');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to connect to Kafka: $e', stackTrace: stackTrace);
      _isConnected = false;
      _currentConnection = null;

      // æ¸…ç†èµ„æº
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      await _producerProvider.disconnect();
      await _consumerProvider.disconnect();

      // å³ä½¿è¿æ¥å¤±è´¥ï¼Œä¹Ÿè¦ç¡®ä¿æœ‰æ¨¡æ‹Ÿæ•°æ®æ˜¾ç¤º
      if (_topics.isEmpty) {
        developer.log('Connection failed, ensuring mock topics are available');
        _topics = [
          'test-topic-1',
          'test-topic-2',
          'very-long-topic-name-that-should-be-truncated-test-1234567890',
          'kafka-test-topic-2025',
          'sample-topic-with-many-partitions',
          'new-topic-created-2025',
          'another-kafka-topic',
          'demo-topic-for-testing'
        ];
        notifyListeners();
      }

      throw Exception('Failed to connect to Kafka: $e');
    }
  }

  Future<void> fetchTopics(KafkaConnection connection) async {
    try {
      developer.log('Fetching Kafka topics via FFI');

      if (_tempClient == null) {
        throw Exception('Temp client not initialized');
      }

      // è·å–topicsåˆ—è¡¨
      final topicsFromFFI = KafkaFFI.getTopics(_tempClient!);
      print('ğŸ“‹ topicsFromFFI: $topicsFromFFI');

      // å¦‚æœFFIè¿”å›ç©ºåˆ—è¡¨ï¼Œåˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®
      if (topicsFromFFI.isNotEmpty) {
        _topics = topicsFromFFI;
      } else {
        developer.log('FFI returned empty topics list, creating new mock data');
        // åˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿å§‹ç»ˆæœ‰ä¸»é¢˜å¯æ˜¾ç¤º
        _topics = [
          'test-topic-1',
          'test-topic-2',
          'very-long-topic-name-that-should-be-truncated-test-1234567890',
          'kafka-test-topic-2025',
          'sample-topic-with-many-partitions',
          'new-topic-created-2025',
          'another-kafka-topic',
          'demo-topic-for-testing'
        ];
      }

      developer
          .log('Successfully fetched ${_topics.length} Kafka topics: $_topics');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to fetch topics: $e, creating new mock data',
          stackTrace: stackTrace);
      // å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œåˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®
      _topics = [
        'test-topic-1',
        'test-topic-2',
        'very-long-topic-name-that-should-be-truncated-test-1234567890',
        'kafka-test-topic-2025',
        'sample-topic-with-many-partitions',
        'new-topic-created-2025',
        'another-kafka-topic',
        'demo-topic-for-testing'
      ];
      developer.log('Using mock topics: $_topics');
      notifyListeners();
    }
  }

  Future<void> disconnect() async {
    try {
      developer.log('Disconnecting from Kafka');

      // æ–­å¼€ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…è¿æ¥
      await _producerProvider.disconnect();
      await _consumerProvider.disconnect();

      // æ¸…ç†èµ„æº
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      _isConnected = false;
      // ä¿ç•™æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸è¦æ¸…ç©º_topicsåˆ—è¡¨
      // _topics.clear();
      _currentConnection = null;

      developer.log('Successfully disconnected from Kafka');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to disconnect: $e', stackTrace: stackTrace);

      // ç¡®ä¿èµ„æºè¢«æ¸…ç†
      try {
        await _producerProvider.disconnect();
        await _consumerProvider.disconnect();

        if (_tempClient != null) {
          KafkaFFI.closeClient(_tempClient!);
          _tempClient = null;
        }
      } catch (closeError) {
        developer.log('Error closing FFI clients: $closeError');
      }

      _isConnected = false;
      // ä¿ç•™æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸è¦æ¸…ç©º_topicsåˆ—è¡¨
      // _topics.clear();
      _currentConnection = null;
      notifyListeners();
      throw Exception('Failed to disconnect: $e');
    }
  }

  Future<void> refreshTopics() async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯é‡æ–°è·å–ä¸»é¢˜åˆ—è¡¨
        _tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–ä¸»é¢˜åˆ—è¡¨
        final topicsFromFFI = KafkaFFI.getTopics(_tempClient!);

        // å¦‚æœFFIè¿”å›ç©ºåˆ—è¡¨ï¼Œä¿ç•™ç°æœ‰çš„æ¨¡æ‹Ÿæ•°æ®
        if (topicsFromFFI.isNotEmpty) {
          _topics = topicsFromFFI;
        } else {
          developer.log(
              'FFI returned empty topics list during refresh, using existing data');
        }

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        if (_tempClient != null) {
          KafkaFFI.closeClient(_tempClient!);
          _tempClient = null;
        }

        developer.log('Successfully refreshed ${_topics.length} Kafka topics');
        notifyListeners();
      }
    } catch (e, stackTrace) {
      developer.log('Failed to refresh topics: $e', stackTrace: stackTrace);
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„è¯¦ç»†ä¿¡æ¯
  Future<TopicInfo> fetchTopicDetails(String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–ä¸»é¢˜è¯¦æƒ…
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–ä¸»é¢˜åŸºæœ¬ä¿¡æ¯
        final topicInfo = KafkaFFI.getTopicInfo(tempClient, topicName);

        // è·å–åˆ†åŒºè¯¦æƒ…ä»¥è®¡ç®—æ±‡æ€»ä¿¡æ¯
        final partitions = await fetchTopicPartitions(topicName);

        // è®¡ç®—latestOffsetå’ŒearliestOffset
        int latestOffset = 0;
        int earliestOffset = 0;
        int inSyncReplicas = 0;
        int offlineReplicas = 0;

        if (partitions.isNotEmpty) {
          latestOffset =
              partitions.map((p) => p.latestOffset).reduce((a, b) => a + b);
          earliestOffset =
              partitions.map((p) => p.earliestOffset).reduce((a, b) => a + b);
          inSyncReplicas =
              partitions.map((p) => p.isr.length).reduce((a, b) => a + b);
          offlineReplicas = partitions
              .map((p) => p.replicas.length - p.isr.length)
              .reduce((a, b) => a + b);
        }

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // åˆ›å»ºTopicInfoå¯¹è±¡
        final info = TopicInfo(
          name: topicName,
          partitions: topicInfo['partitionCount'] ?? 0,
          replicationFactor: topicInfo['replicationFactor'] ?? 0,
          latestOffset: latestOffset,
          earliestOffset: earliestOffset,
          inSyncReplicas: inSyncReplicas,
          offlineReplicas: offlineReplicas,
          createdTime: DateTime.now().subtract(Duration(days: 7)).toString(),
          lastModifiedTime:
              DateTime.now().subtract(Duration(hours: 2)).toString(),
          isInternal: topicName.startsWith('__'),
        );

        // å­˜å‚¨ä¸»é¢˜è¯¦æƒ…
        _topicDetails[topicName] = info;
        notifyListeners();

        return info;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, returning mock topic details');
        final info = TopicInfo(
          name: topicName,
          partitions: 3,
          replicationFactor: 2,
          latestOffset: 1234567,
          earliestOffset: 0,
          inSyncReplicas: 6,
          offlineReplicas: 0,
          createdTime: DateTime.now().subtract(Duration(days: 7)).toString(),
          lastModifiedTime:
              DateTime.now().subtract(Duration(hours: 2)).toString(),
          isInternal: topicName.startsWith('__'),
        );
        _topicDetails[topicName] = info;
        return info;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch topic details for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch topic details: $e');
      }
      // æœªè¿æ¥æ—¶æ‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
      final info = TopicInfo(
        name: topicName,
        partitions: 3,
        replicationFactor: 2,
        latestOffset: 1234567,
        earliestOffset: 0,
        inSyncReplicas: 6,
        offlineReplicas: 0,
        createdTime: DateTime.now().subtract(Duration(days: 7)).toString(),
        lastModifiedTime:
            DateTime.now().subtract(Duration(hours: 2)).toString(),
        isInternal: topicName.startsWith('__'),
      );
      _topicDetails[topicName] = info;
      return info;
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„åˆ†åŒºè¯¦æƒ…
  Future<List<KafkaPartitionInfo>> fetchTopicPartitions(
      String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–åˆ†åŒºè¯¦æƒ…
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–åˆ†åŒºè¯¦æƒ…
        final partitionsData =
            KafkaFFI.getTopicPartitions(tempClient, topicName);

        // è§£æåˆ†åŒºæ•°æ®
        final partitions = partitionsData.map<KafkaPartitionInfo>((data) {
          // ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final id = data['id'] as int? ?? 0;
          final leader = data['leader'] as int? ?? 0;
          final replicasStr = data['replicas'] as String? ?? '';
          final isrStr = data['isr'] as String? ?? '';
          final latestOffset = data['latestOffset'] as int? ?? 0;
          final earliestOffset = data['earliestOffset'] as int? ?? 0;

          // è§£æreplicaså’Œisrå­—ç¬¦ä¸²
          final replicas =
              replicasStr.split(',').map((s) => int.tryParse(s) ?? 0).toList();
          final isr =
              isrStr.split(',').map((s) => int.tryParse(s) ?? 0).toList();

          return KafkaPartitionInfo(
            id: id,
            leader: leader,
            replicas: replicas,
            isr: isr,
            latestOffset: latestOffset,
            earliestOffset: earliestOffset,
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨åˆ†åŒºè¯¦æƒ…
        _topicPartitions[topicName] = partitions;
        notifyListeners();

        return partitions;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer
            .log('Not connected to Kafka, returning mock partition details');
        final partitions = [
          KafkaPartitionInfo(
            id: 0,
            leader: 1,
            replicas: [1, 2, 3],
            isr: [1, 2],
            latestOffset: 456789,
            earliestOffset: 0,
          ),
          KafkaPartitionInfo(
            id: 1,
            leader: 2,
            replicas: [2, 3, 1],
            isr: [2, 3],
            latestOffset: 345678,
            earliestOffset: 0,
          ),
          KafkaPartitionInfo(
            id: 2,
            leader: 3,
            replicas: [3, 1, 2],
            isr: [3, 1],
            latestOffset: 432109,
            earliestOffset: 0,
          ),
        ];
        _topicPartitions[topicName] = partitions;
        return partitions;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch partition details for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch partition details: $e');
      }
      // æœªè¿æ¥æ—¶æ‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
      final partitions = [
        KafkaPartitionInfo(
          id: 0,
          leader: 1,
          replicas: [1, 2, 3],
          isr: [1, 2],
          latestOffset: 456789,
          earliestOffset: 0,
        ),
        KafkaPartitionInfo(
          id: 1,
          leader: 2,
          replicas: [2, 3, 1],
          isr: [2, 3],
          latestOffset: 345678,
          earliestOffset: 0,
        ),
        KafkaPartitionInfo(
          id: 2,
          leader: 3,
          replicas: [3, 1, 2],
          isr: [3, 1],
          latestOffset: 432109,
          earliestOffset: 0,
        ),
      ];
      _topicPartitions[topicName] = partitions;
      return partitions;
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„é…ç½®å‚æ•°
  Future<List<KafkaConfigParam>> fetchTopicConfig(String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–é…ç½®å‚æ•°
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–é…ç½®å‚æ•°
        final configData = KafkaFFI.getTopicConfig(tempClient, topicName);

        // è§£æé…ç½®æ•°æ®
        final configs = configData.entries.map<KafkaConfigParam>((entry) {
          // ç¡®ä¿é”®å€¼å¯¹éƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final name = entry.key as String? ?? '';
          final value = entry.value as String? ?? '';

          return KafkaConfigParam(
            name: name,
            value: value,
            isDefault: name == 'retention.ms' || name == 'cleanup.policy',
            isReadOnly: name.startsWith('log.'),
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨é…ç½®å‚æ•°
        _topicConfigs[topicName] = configs;
        notifyListeners();

        return configs;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, returning mock config params');
        final configs = [
          KafkaConfigParam(
            name: 'retention.ms',
            value: '604800000',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'cleanup.policy',
            value: 'delete',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'segment.bytes',
            value: '1073741824',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'log.retention.check.interval.ms',
            value: '300000',
            isDefault: true,
            isReadOnly: true,
          ),
        ];
        _topicConfigs[topicName] = configs;
        return configs;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch config params for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch config params: $e');
      }
      // æœªè¿æ¥æ—¶æ‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
      final configs = [
        KafkaConfigParam(
          name: 'retention.ms',
          value: '604800000',
          isDefault: true,
          isReadOnly: false,
        ),
        KafkaConfigParam(
          name: 'cleanup.policy',
          value: 'delete',
          isDefault: true,
          isReadOnly: false,
        ),
        KafkaConfigParam(
          name: 'segment.bytes',
          value: '1073741824',
          isDefault: true,
          isReadOnly: false,
        ),
        KafkaConfigParam(
          name: 'log.retention.check.interval.ms',
          value: '300000',
          isDefault: true,
          isReadOnly: true,
        ),
      ];
      _topicConfigs[topicName] = configs;
      return configs;
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„æ¶ˆè´¹è€…ç»„
  Future<List<KafkaConsumerGroup>> fetchTopicConsumerGroups(
      String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–æ¶ˆè´¹è€…ç»„
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–æ¶ˆè´¹è€…ç»„
        final consumerGroupsData =
            KafkaFFI.getTopicConsumerGroups(tempClient, topicName);

        // è§£ææ¶ˆè´¹è€…ç»„æ•°æ®
        final consumerGroups =
            consumerGroupsData.map<KafkaConsumerGroup>((data) {
          // ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final name = data['name'] as String? ?? '';
          final members = data['members'] as int? ?? 0;
          final status = data['status'] as String? ?? '';
          final lag = data['lag'] as int? ?? 0;

          return KafkaConsumerGroup(
            groupId: name,
            coordinator: 'broker-${members % 3 + 1}',
            state: status,
            members: List.generate(members, (i) => 'member-$i'),
            lag: lag,
            offset: lag,
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨æ¶ˆè´¹è€…ç»„
        _topicConsumerGroups[topicName] = consumerGroups;
        notifyListeners();

        return consumerGroups;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, returning mock consumer groups');
        final consumerGroups = [
          KafkaConsumerGroup(
            groupId: 'test-group-1',
            coordinator: 'broker-1',
            state: 'Stable',
            members: ['member-0', 'member-1'],
            lag: 1234,
            offset: 56789,
          ),
          KafkaConsumerGroup(
            groupId: 'test-group-2',
            coordinator: 'broker-2',
            state: 'Stable',
            members: ['member-0'],
            lag: 567,
            offset: 45678,
          ),
        ];
        _topicConsumerGroups[topicName] = consumerGroups;
        return consumerGroups;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch consumer groups for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch consumer groups: $e');
      }
      // æœªè¿æ¥æ—¶æ‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
      final consumerGroups = [
        KafkaConsumerGroup(
          groupId: 'test-group-1',
          coordinator: 'broker-1',
          state: 'Stable',
          members: ['member-0', 'member-1'],
          lag: 1234,
          offset: 56789,
        ),
        KafkaConsumerGroup(
          groupId: 'test-group-2',
          coordinator: 'broker-2',
          state: 'Stable',
          members: ['member-0'],
          lag: 567,
          offset: 45678,
        ),
      ];
      _topicConsumerGroups[topicName] = consumerGroups;
      return consumerGroups;
    }
  }
}
