import 'package:flutter/material.dart';
import 'dart:developer' as developer;
import 'package:shared_preferences/shared_preferences.dart';
import './producer_provider.dart';
import './consumer_provider.dart';
import '../ffi/kafka_ffi.dart';
import '../models/topic_model.dart';

class KafkaConnection {
  final String name;
  final String bootstrapServers;

  KafkaConnection({
    required this.name,
    required this.bootstrapServers,
  });

  String get servers => bootstrapServers;

  Map<String, dynamic> toMap() {
    return {
      'name': name,
      'bootstrapServers': bootstrapServers,
    };
  }

  factory KafkaConnection.fromMap(Map<String, dynamic> map) {
    return KafkaConnection(
      name: map['name'],
      bootstrapServers: map['bootstrapServers'],
    );
  }
}

class KafkaProvider extends ChangeNotifier {
  bool _isConnected = false;
  List<String> _topics = [
    'test-topic-1',
    'test-topic-2',
    'very-long-topic-name-that-should-be-truncated-test-1234567890',
    'kafka-test-topic-2025',
    'sample-topic-with-many-partitions',
    'new-topic-created-2025',
    'another-kafka-topic',
    'demo-topic-for-testing'
  ];
  List<KafkaConnection> _savedConnections = [];
  KafkaConnection? _currentConnection;
  KafkaClientHandle? _tempClient;

  // å­˜å‚¨ä¸»é¢˜è¯¦æƒ…çš„æ˜ å°„
  Map<String, TopicInfo> _topicDetails = {};
  Map<String, List<KafkaPartitionInfo>> _topicPartitions = {};
  Map<String, List<KafkaConfigParam>> _topicConfigs = {};
  Map<String, List<KafkaConsumerGroup>> _topicConsumerGroups = {};

  // åŠ è½½çŠ¶æ€
  bool _isLoadingTopicDetails = false;
  String? _loadingTopic;

  // å­Provider
  final ProducerProvider _producerProvider = ProducerProvider();
  final ConsumerProvider _consumerProvider = ConsumerProvider();

  // æ„é€ å‡½æ•°
  KafkaProvider() {
    // ç›‘å¬å­Providerçš„å˜åŒ–ï¼Œå½“å®ƒä»¬å˜åŒ–æ—¶é€šçŸ¥è‡ªå·±çš„ç›‘å¬å™¨
    _producerProvider.addListener(() {
      notifyListeners();
    });
    _consumerProvider.addListener(() {
      notifyListeners();
    });
  }

  // Getters
  List<KafkaConnection> get savedConnections => _savedConnections;
  KafkaConnection? get currentConnection => _currentConnection;
  bool get isConnected => _isConnected;
  List<String> get topics => _topics;
  ProducerProvider get producerProvider => _producerProvider;
  ConsumerProvider get consumerProvider => _consumerProvider;
  Map<String, TopicInfo> get topicDetails => _topicDetails;
  Map<String, List<KafkaPartitionInfo>> get topicPartitions => _topicPartitions;
  Map<String, List<KafkaConfigParam>> get topicConfigs => _topicConfigs;
  Map<String, List<KafkaConsumerGroup>> get topicConsumerGroups =>
      _topicConsumerGroups;
  bool get isLoadingTopicDetails => _isLoadingTopicDetails;
  String? get loadingTopic => _loadingTopic;

  Future<void> loadSavedConnections() async {
    try {
      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = prefs.getStringList('kafka_connections');

      if (connectionsJson != null) {
        _savedConnections = connectionsJson.map((json) {
          final parts = json.split('|||');
          return KafkaConnection.fromMap({
            'name': parts[0],
            'bootstrapServers': parts[1],
          });
        }).toList();
      }

      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to load saved connections: $e',
          stackTrace: stackTrace);
    }
  }

  Future<void> saveConnection(KafkaConnection connection) async {
    try {
      final existingIndex = _savedConnections.indexWhere(
        (c) => c.name == connection.name,
      );

      if (existingIndex != -1) {
        _savedConnections[existingIndex] = connection;
      } else {
        _savedConnections.add(connection);
      }

      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = _savedConnections
          .map((c) => '${c.name}|||${c.bootstrapServers}')
          .toList();

      await prefs.setStringList('kafka_connections', connectionsJson);
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to save connection: $e', stackTrace: stackTrace);
      throw Exception('Failed to save connection: $e');
    }
  }

  Future<void> deleteConnection(String connectionName) async {
    try {
      _savedConnections.removeWhere((c) => c.name == connectionName);

      final prefs = await SharedPreferences.getInstance();
      final connectionsJson = _savedConnections
          .map((c) => '${c.name}|||${c.bootstrapServers}')
          .toList();

      await prefs.setStringList('kafka_connections', connectionsJson);
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to delete connection: $e', stackTrace: stackTrace);
      throw Exception('Failed to delete connection: $e');
    }
  }

  /// æµ‹è¯•ä¸Kafkaé›†ç¾¤çš„è¿æ¥
  Future<bool> testConnection(String bootstrapServers) async {
    try {
      developer.log('Testing connection to Kafka at $bootstrapServers via FFI');

      // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
      final tempClient = KafkaFFI.createProducer(bootstrapServers);

      // å°è¯•è·å–ä¸»é¢˜åˆ—è¡¨ï¼ŒéªŒè¯è¿æ¥æ˜¯å¦æˆåŠŸ
      final topics = KafkaFFI.getTopics(tempClient);

      // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
      KafkaFFI.closeClient(tempClient);

      developer
          .log('Connection test successful. Found ${topics.length} topics');
      return true;
    } catch (e, stackTrace) {
      developer.log('Connection test failed: $e', stackTrace: stackTrace);
      return false;
    }
  }

  Future<void> connect(String bootstrapServers,
      {String? connectionName}) async {
    try {
      KafkaConnection connection = KafkaConnection(
        name: connectionName ?? 'ä¸´æ—¶è¿æ¥',
        bootstrapServers: bootstrapServers,
      );

      developer.log(
          'Attempting to connect to Kafka at ${connection.bootstrapServers} via FFI');

      // ä½¿ç”¨ä¸´æ—¶å®¢æˆ·ç«¯è·å–ä¸»é¢˜åˆ—è¡¨
      _tempClient = KafkaFFI.createProducer(connection.bootstrapServers);

      // æ·»åŠ å»¶è¿Ÿï¼Œç¡®ä¿å®¢æˆ·ç«¯æœ‰è¶³å¤Ÿçš„æ—¶é—´è¿æ¥åˆ°Kafkaé›†ç¾¤
      developer.log('Waiting for Kafka client to connect...');
      await Future.delayed(const Duration(seconds: 1));

      await fetchTopics(connection);

      // è¿æ¥ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
      await _producerProvider.connect(connection.bootstrapServers);
      await _consumerProvider.connect(connection.bootstrapServers);

      _isConnected = true;
      _currentConnection = connection;

      // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      developer.log(
          'Successfully connected to Kafka at ${connection.bootstrapServers}');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to connect to Kafka: $e', stackTrace: stackTrace);
      _isConnected = false;
      _currentConnection = null;

      // æ¸…ç†èµ„æº
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      await _producerProvider.disconnect();
      await _consumerProvider.disconnect();

      // å³ä½¿è¿æ¥å¤±è´¥ï¼Œä¹Ÿè¦ç¡®ä¿æœ‰æ¨¡æ‹Ÿæ•°æ®æ˜¾ç¤º
      if (_topics.isEmpty) {
        developer.log('Connection failed, ensuring mock topics are available');
        _topics = [
          'test-topic-1',
          'test-topic-2',
          'very-long-topic-name-that-should-be-truncated-test-1234567890',
          'kafka-test-topic-2025',
          'sample-topic-with-many-partitions',
          'new-topic-created-2025',
          'another-kafka-topic',
          'demo-topic-for-testing'
        ];
        notifyListeners();
      }

      throw Exception('Failed to connect to Kafka: $e');
    }
  }

  Future<void> fetchTopics(KafkaConnection connection) async {
    try {
      developer.log('Fetching Kafka topics via FFI');

      if (_tempClient == null) {
        throw Exception('Temp client not initialized');
      }

      // è·å–topicsåˆ—è¡¨
      final topicsFromFFI = KafkaFFI.getTopics(_tempClient!);
      print('ğŸ“‹ topicsFromFFI: $topicsFromFFI');

      // å¦‚æœFFIè¿”å›ç©ºåˆ—è¡¨ï¼Œåˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®
      if (topicsFromFFI.isNotEmpty) {
        _topics = topicsFromFFI;
      } else {
        developer.log('FFI returned empty topics list, creating new mock data');
        // åˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿å§‹ç»ˆæœ‰ä¸»é¢˜å¯æ˜¾ç¤º
        _topics = [
          'test-topic-1',
          'test-topic-2',
          'very-long-topic-name-that-should-be-truncated-test-1234567890',
          'kafka-test-topic-2025',
          'sample-topic-with-many-partitions',
          'new-topic-created-2025',
          'another-kafka-topic',
          'demo-topic-for-testing'
        ];
      }

      developer
          .log('Successfully fetched ${_topics.length} Kafka topics: $_topics');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to fetch topics: $e, creating new mock data',
          stackTrace: stackTrace);
      // å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œåˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®
      _topics = [
        'test-topic-1',
        'test-topic-2',
        'very-long-topic-name-that-should-be-truncated-test-1234567890',
        'kafka-test-topic-2025',
        'sample-topic-with-many-partitions',
        'new-topic-created-2025',
        'another-kafka-topic',
        'demo-topic-for-testing'
      ];
      developer.log('Using mock topics: $_topics');
      notifyListeners();
    }
  }

  Future<void> disconnect() async {
    try {
      developer.log('Disconnecting from Kafka');

      // æ–­å¼€ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…è¿æ¥
      await _producerProvider.disconnect();
      await _consumerProvider.disconnect();

      // æ¸…ç†èµ„æº
      if (_tempClient != null) {
        KafkaFFI.closeClient(_tempClient!);
        _tempClient = null;
      }

      _isConnected = false;
      // ä¿ç•™æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸è¦æ¸…ç©º_topicsåˆ—è¡¨
      // _topics.clear();
      _currentConnection = null;

      developer.log('Successfully disconnected from Kafka');
      notifyListeners();
    } catch (e, stackTrace) {
      developer.log('Failed to disconnect: $e', stackTrace: stackTrace);

      // ç¡®ä¿èµ„æºè¢«æ¸…ç†
      try {
        await _producerProvider.disconnect();
        await _consumerProvider.disconnect();

        if (_tempClient != null) {
          KafkaFFI.closeClient(_tempClient!);
          _tempClient = null;
        }
      } catch (closeError) {
        developer.log('Error closing FFI clients: $closeError');
      }

      _isConnected = false;
      // ä¿ç•™æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸è¦æ¸…ç©º_topicsåˆ—è¡¨
      // _topics.clear();
      _currentConnection = null;
      notifyListeners();
      throw Exception('Failed to disconnect: $e');
    }
  }

  Future<void> refreshTopics() async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯é‡æ–°è·å–ä¸»é¢˜åˆ—è¡¨
        _tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–ä¸»é¢˜åˆ—è¡¨
        final topicsFromFFI = KafkaFFI.getTopics(_tempClient!);

        // å¦‚æœFFIè¿”å›ç©ºåˆ—è¡¨ï¼Œä¿ç•™ç°æœ‰çš„æ¨¡æ‹Ÿæ•°æ®
        if (topicsFromFFI.isNotEmpty) {
          _topics = topicsFromFFI;
        } else {
          developer.log(
              'FFI returned empty topics list during refresh, using existing data');
        }

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        if (_tempClient != null) {
          KafkaFFI.closeClient(_tempClient!);
          _tempClient = null;
        }

        developer.log('Successfully refreshed ${_topics.length} Kafka topics');
        notifyListeners();
      }
    } catch (e, stackTrace) {
      developer.log('Failed to refresh topics: $e', stackTrace: stackTrace);
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„è¯¦ç»†ä¿¡æ¯
  Future<TopicInfo> fetchTopicDetails(String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–ä¸»é¢˜è¯¦æƒ…
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–ä¸»é¢˜åŸºæœ¬ä¿¡æ¯
        final topicInfo = KafkaFFI.getTopicInfo(tempClient, topicName);

        // è·å–åˆ†åŒºè¯¦æƒ…ä»¥è®¡ç®—æ±‡æ€»ä¿¡æ¯
        final partitions = await fetchTopicPartitions(topicName);

        // è®¡ç®—latestOffsetå’ŒearliestOffset
        int latestOffset = 0;
        int earliestOffset = 0;
        int inSyncReplicas = 0;
        int offlineReplicas = 0;

        if (partitions.isNotEmpty) {
          latestOffset =
              partitions.map((p) => p.latestOffset).reduce((a, b) => a + b);
          earliestOffset =
              partitions.map((p) => p.earliestOffset).reduce((a, b) => a + b);
          inSyncReplicas =
              partitions.map((p) => p.isr.length).reduce((a, b) => a + b);
          offlineReplicas = partitions
              .map((p) => p.replicas.length - p.isr.length)
              .reduce((a, b) => a + b);
        }

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // åˆ›å»ºTopicInfoå¯¹è±¡
        final info = TopicInfo(
          name: topicName,
          partitions: topicInfo['partitionCount'] ?? 0,
          replicationFactor: topicInfo['replicationFactor'] ?? 0,
          latestOffset: latestOffset,
          earliestOffset: earliestOffset,
          inSyncReplicas: inSyncReplicas,
          offlineReplicas: offlineReplicas,
          createdTime: DateTime.now().subtract(Duration(days: 7)).toString(),
          lastModifiedTime:
              DateTime.now().subtract(Duration(hours: 2)).toString(),
          isInternal: topicName.startsWith('__'),
        );

        // å­˜å‚¨ä¸»é¢˜è¯¦æƒ…
        _topicDetails[topicName] = info;
        notifyListeners();

        return info;
      } else {
        // è¿”å›è¿æ¥çŠ¶æ€ä¿¡æ¯è€Œéå›ºå®šæ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, showing connection status');
        final info = TopicInfo(
          name: topicName,
          partitions: 0,
          replicationFactor: 0,
          latestOffset: 0,
          earliestOffset: 0,
          inSyncReplicas: 0,
          offlineReplicas: 0,
          createdTime: 'N/A',
          lastModifiedTime: 'N/A',
          isInternal: topicName.startsWith('__'),
        );
        _topicDetails[topicName] = info;
        return info;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch topic details for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch topic details: $e');
      }
      // æœªè¿æ¥æ—¶æ‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
      final info = TopicInfo(
        name: topicName,
        partitions: 3,
        replicationFactor: 2,
        latestOffset: 1234567,
        earliestOffset: 0,
        inSyncReplicas: 6,
        offlineReplicas: 0,
        createdTime: DateTime.now().subtract(Duration(days: 7)).toString(),
        lastModifiedTime:
            DateTime.now().subtract(Duration(hours: 2)).toString(),
        isInternal: topicName.startsWith('__'),
      );
      _topicDetails[topicName] = info;
      return info;
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„åˆ†åŒºè¯¦æƒ…
  Future<List<KafkaPartitionInfo>> fetchTopicPartitions(
      String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–åˆ†åŒºè¯¦æƒ…
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–åˆ†åŒºè¯¦æƒ…
        final partitionsData =
            KafkaFFI.getTopicPartitions(tempClient, topicName);

        // è§£æåˆ†åŒºæ•°æ®
        final partitions = partitionsData.map<KafkaPartitionInfo>((data) {
          // ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final id = data['id'] as int? ?? 0;
          final leader = data['leader'] as int? ?? 0;
          final replicasStr = data['replicas'] as String? ?? '';
          final isrStr = data['isr'] as String? ?? '';
          final latestOffset = data['latestOffset'] as int? ?? 0;
          final earliestOffset = data['earliestOffset'] as int? ?? 0;

          // è§£æreplicaså’Œisrå­—ç¬¦ä¸²
          final replicas =
              replicasStr.split(',').map((s) => int.tryParse(s) ?? 0).toList();
          final isr =
              isrStr.split(',').map((s) => int.tryParse(s) ?? 0).toList();

          return KafkaPartitionInfo(
            id: id,
            leader: leader,
            replicas: replicas,
            isr: isr,
            latestOffset: latestOffset,
            earliestOffset: earliestOffset,
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨åˆ†åŒºè¯¦æƒ…
        _topicPartitions[topicName] = partitions;
        notifyListeners();

        return partitions;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer
            .log('Not connected to Kafka, returning mock partition details');
        final partitions = [
          KafkaPartitionInfo(
            id: 0,
            leader: 1,
            replicas: [1, 2, 3],
            isr: [1, 2],
            latestOffset: 456789,
            earliestOffset: 0,
          ),
          KafkaPartitionInfo(
            id: 1,
            leader: 2,
            replicas: [2, 3, 1],
            isr: [2, 3],
            latestOffset: 345678,
            earliestOffset: 0,
          ),
          KafkaPartitionInfo(
            id: 2,
            leader: 3,
            replicas: [3, 1, 2],
            isr: [3, 1],
            latestOffset: 432109,
            earliestOffset: 0,
          ),
        ];
        _topicPartitions[topicName] = partitions;
        return partitions;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch partition details for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch partition details: $e');
      }
      // æœªè¿æ¥æ—¶è¿”å›ç©ºåˆ—è¡¨
      _topicPartitions[topicName] = [];
      return [];
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„é…ç½®å‚æ•°
  Future<List<KafkaConfigParam>> fetchTopicConfig(String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–é…ç½®å‚æ•°
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–é…ç½®å‚æ•°
        final configData = KafkaFFI.getTopicConfig(tempClient, topicName);

        // è§£æé…ç½®æ•°æ®
        final configs = configData.entries.map<KafkaConfigParam>((entry) {
          // ç¡®ä¿é”®å€¼å¯¹éƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final name = entry.key as String? ?? '';
          final value = entry.value as String? ?? '';

          return KafkaConfigParam(
            name: name,
            value: value,
            isDefault: name == 'retention.ms' || name == 'cleanup.policy',
            isReadOnly: name.startsWith('log.'),
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨é…ç½®å‚æ•°
        _topicConfigs[topicName] = configs;
        notifyListeners();

        return configs;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, returning mock config params');
        final configs = [
          KafkaConfigParam(
            name: 'retention.ms',
            value: '604800000',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'cleanup.policy',
            value: 'delete',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'segment.bytes',
            value: '1073741824',
            isDefault: true,
            isReadOnly: false,
          ),
          KafkaConfigParam(
            name: 'log.retention.check.interval.ms',
            value: '300000',
            isDefault: true,
            isReadOnly: true,
          ),
        ];
        _topicConfigs[topicName] = configs;
        return configs;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch config params for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch config params: $e');
      }
      // æœªè¿æ¥æ—¶è¿”å›ç©ºåˆ—è¡¨
      _topicConfigs[topicName] = [];
      return [];
    }
  }

  // è·å–æŒ‡å®šä¸»é¢˜çš„æ¶ˆè´¹è€…ç»„
  Future<List<KafkaConsumerGroup>> fetchTopicConsumerGroups(
      String topicName) async {
    try {
      if (_isConnected && _currentConnection != null) {
        // åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è·å–æ¶ˆè´¹è€…ç»„
        final tempClient =
            KafkaFFI.createProducer(_currentConnection!.bootstrapServers);

        // è·å–æ¶ˆè´¹è€…ç»„
        final consumerGroupsData =
            KafkaFFI.getTopicConsumerGroups(tempClient, topicName);

        // è§£ææ¶ˆè´¹è€…ç»„æ•°æ®
        final consumerGroups =
            consumerGroupsData.map<KafkaConsumerGroup>((data) {
          // ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
          final name = data['name'] as String? ?? '';
          final members = data['members'] as int? ?? 0;
          final status = data['status'] as String? ?? '';
          final lag = data['lag'] as int? ?? 0;

          return KafkaConsumerGroup(
            groupId: name,
            coordinator: 'broker-${members % 3 + 1}',
            state: status,
            members: List.generate(members, (i) => 'member-$i'),
            lag: lag,
            offset: lag,
          );
        }).toList();

        // å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
        KafkaFFI.closeClient(tempClient);

        // å­˜å‚¨æ¶ˆè´¹è€…ç»„
        _topicConsumerGroups[topicName] = consumerGroups;
        notifyListeners();

        return consumerGroups;
      } else {
        // è¿”å›æ¨¡æ‹Ÿæ•°æ®
        developer.log('Not connected to Kafka, returning mock consumer groups');
        final consumerGroups = [
          KafkaConsumerGroup(
            groupId: 'test-group-1',
            coordinator: 'broker-1',
            state: 'Stable',
            members: ['member-0', 'member-1'],
            lag: 1234,
            offset: 56789,
          ),
          KafkaConsumerGroup(
            groupId: 'test-group-2',
            coordinator: 'broker-2',
            state: 'Stable',
            members: ['member-0'],
            lag: 567,
            offset: 45678,
          ),
        ];
        _topicConsumerGroups[topicName] = consumerGroups;
        return consumerGroups;
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch consumer groups for $topicName: $e',
          stackTrace: stackTrace);
      // å¦‚æœå·²ç»è¿æ¥ä½†è·å–æ•°æ®å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›æ¨¡æ‹Ÿæ•°æ®
      if (_isConnected && _currentConnection != null) {
        throw Exception('Failed to fetch consumer groups: $e');
      }
      // æœªè¿æ¥æ—¶è¿”å›ç©ºåˆ—è¡¨
      _topicConsumerGroups[topicName] = [];
      return [];
    }
  }

  /// ä¸€æ¬¡æ€§è·å–ä¸»é¢˜çš„æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
  /// åªåˆ›å»ºä¸€ä¸ªä¸´æ—¶å®¢æˆ·ç«¯ï¼Œå¹¶è¡Œè·å–æ‰€æœ‰æ•°æ®
  Future<void> fetchAllTopicInfo(String topicName, {bool forceRefresh = false}) async {
    developer.log('fetchAllTopicInfo called for topic: $topicName, forceRefresh: $forceRefresh');
    developer.log('Current state: isConnected=$_isConnected, currentConnection=$_currentConnection');

    // å¦‚æœå·²ç»æœ‰ç¼“å­˜æ•°æ®ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œç›´æ¥è¿”å›
    if (!forceRefresh &&
        _topicDetails.containsKey(topicName) &&
        _topicPartitions.containsKey(topicName) &&
        _topicConfigs.containsKey(topicName) &&
        _topicConsumerGroups.containsKey(topicName)) {
      developer.log('Using cached data for topic: $topicName');
      return;
    }

    // è®¾ç½®åŠ è½½çŠ¶æ€
    _isLoadingTopicDetails = true;
    _loadingTopic = topicName;
    notifyListeners();

    try {
      if (_isConnected && _currentConnection != null) {
        developer.log('Connected, fetching real data from Kafka...');
        // åˆ›å»ºä¸€ä¸ªä¸´æ—¶å®¢æˆ·ç«¯ï¼Œç”¨äºæ‰€æœ‰è¯·æ±‚
        final tempClient = KafkaFFI.createProducer(_currentConnection!.bootstrapServers);
        developer.log('Created temp client: $tempClient');

        try {
          // è·å–åˆ†åŒºã€é…ç½®å’Œæ¶ˆè´¹è€…ç»„æ•°æ®
          List<Map<String, dynamic>> partitionsData = [];
          Map<String, String> configData = {};
          List<Map<String, dynamic>> consumerGroupsData = [];
          Map<String, int> topicInfo = {'partitionCount': 0, 'replicationFactor': 0};

          // é€æ­¥è·å–æ•°æ®ï¼Œå¯¹å¯èƒ½å¤±è´¥çš„æ“ä½œè¿›è¡Œå•ç‹¬å¤„ç†
          try {
            developer.log('Fetching partitions data...');
            partitionsData = KafkaFFI.getTopicPartitions(tempClient, topicName);
            developer.log('Partitions data: $partitionsData');
          } catch (e) {
            developer.log('Failed to fetch partitions: $e');
            partitionsData = []; // ä½¿ç”¨ç©ºåˆ—è¡¨è€Œä¸æ˜¯å¤±è´¥
          }

          try {
            developer.log('Fetching config data...');
            configData = KafkaFFI.getTopicConfig(tempClient, topicName);
            developer.log('Config data: $configData');
          } catch (e) {
            developer.log('Failed to fetch config: $e');
            configData = {}; // ä½¿ç”¨ç©ºæ˜ å°„è€Œä¸æ˜¯å¤±è´¥
          }

          try {
            developer.log('Fetching consumer groups data...');
            consumerGroupsData = KafkaFFI.getTopicConsumerGroups(tempClient, topicName);
            developer.log('Consumer groups data: $consumerGroupsData');
          } catch (e) {
            developer.log('Failed to fetch consumer groups: $e');
            consumerGroupsData = []; // ä½¿ç”¨ç©ºåˆ—è¡¨è€Œä¸æ˜¯å¤±è´¥
          }

          try {
            developer.log('Fetching topic info...');
            topicInfo = KafkaFFI.getTopicInfo(tempClient, topicName);
            developer.log('Topic info: $topicInfo');
          } catch (e) {
            developer.log('Failed to fetch topic info: $e');
            // ä½¿ç”¨é»˜è®¤å€¼
            topicInfo = {'partitionCount': partitionsData.length, 'replicationFactor': 0};
          }

          // è§£æåˆ†åŒºæ•°æ®
          final partitions = partitionsData.map<KafkaPartitionInfo>((data) {
            final id = data['id'] as int? ?? 0;
            final leader = data['leader'] as int? ?? 0;
            final replicasStr = data['replicas'] as String? ?? '';
            final isrStr = data['isr'] as String? ?? '';
            final latestOffset = data['latestOffset'] as int? ?? 0;
            final earliestOffset = data['earliestOffset'] as int? ?? 0;

            final replicas = replicasStr.isNotEmpty
                ? replicasStr.split(',').map((s) => int.tryParse(s.trim()) ?? 0).toList()
                : <int>[];
            final isr = isrStr.isNotEmpty
                ? isrStr.split(',').map((s) => int.tryParse(s.trim()) ?? 0).toList()
                : <int>[];

            return KafkaPartitionInfo(
              id: id,
              leader: leader,
              replicas: replicas,
              isr: isr,
              latestOffset: latestOffset,
              earliestOffset: earliestOffset,
            );
          }).toList();

          // è§£æé…ç½®æ•°æ®
          final configs = configData.entries.map<KafkaConfigParam>((entry) {
            final name = entry.key as String? ?? '';
            final value = entry.value as String? ?? '';
            return KafkaConfigParam(
              name: name,
              value: value,
              isDefault: name == 'retention.ms' || name == 'cleanup.policy',
              isReadOnly: name.startsWith('log.'),
            );
          }).toList();

          // è§£ææ¶ˆè´¹è€…ç»„æ•°æ®
          final consumerGroups = consumerGroupsData.map<KafkaConsumerGroup>((data) {
            final name = data['name'] as String? ?? '';
            final members = data['members'] as int? ?? 0;
            final status = data['status'] as String? ?? '';
            final lag = data['lag'] as int? ?? 0;

            return KafkaConsumerGroup(
              groupId: name,
              coordinator: 'broker-${members % 3 + 1}',
              state: status,
              members: List.generate(members, (i) => 'member-$i'),
              lag: lag,
              offset: lag,
            );
          }).toList();

          // è®¡ç®—æ±‡æ€»ä¿¡æ¯
          int latestOffset = 0;
          int earliestOffset = 0;
          int inSyncReplicas = 0;
          int offlineReplicas = 0;

          if (partitions.isNotEmpty) {
            latestOffset = partitions.map((p) => p.latestOffset).reduce((a, b) => a + b);
            earliestOffset = partitions.map((p) => p.earliestOffset).reduce((a, b) => a + b);
            inSyncReplicas = partitions.map((p) => p.isr.length).reduce((a, b) => a + b);
            offlineReplicas = partitions
                .map((p) => p.replicas.length - p.isr.length)
                .reduce((a, b) => a + b);
          }

          // åˆ›å»º TopicInfo å¯¹è±¡
          final info = TopicInfo(
            name: topicName,
            partitions: topicInfo['partitionCount'] ?? partitions.length,
            replicationFactor: topicInfo['replicationFactor'] ?? 0,
            latestOffset: latestOffset,
            earliestOffset: earliestOffset,
            inSyncReplicas: inSyncReplicas,
            offlineReplicas: offlineReplicas,
            createdTime: DateTime.now().subtract(const Duration(days: 7)).toString(),
            lastModifiedTime: DateTime.now().subtract(const Duration(hours: 2)).toString(),
            isInternal: topicName.startsWith('__'),
          );

          // å­˜å‚¨æ‰€æœ‰æ•°æ®
          _topicDetails[topicName] = info;
          _topicPartitions[topicName] = partitions;
          _topicConfigs[topicName] = configs;
          _topicConsumerGroups[topicName] = consumerGroups;

          developer.log('Successfully fetched all info for topic: $topicName');
        } finally {
          // ç¡®ä¿å…³é—­ä¸´æ—¶å®¢æˆ·ç«¯
          KafkaFFI.closeClient(tempClient);
        }
      } else {
        // å³ä½¿æœªè¿æ¥ä¹Ÿå°è¯•è·å–åŸºæœ¬ä¸»é¢˜ä¿¡æ¯ï¼ˆç”¨äºæ˜¾ç¤ºè¿æ¥çŠ¶æ€ï¼‰
        _setMinimalTopicData(topicName);
      }
    } catch (e, stackTrace) {
      developer.log('Failed to fetch topic info for $topicName: $e', stackTrace: stackTrace);
      // å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨æœ€å°åŒ–æ•°æ®ï¼Œä½†ä¸ä½¿ç”¨å›ºå®šæ•°å€¼
      _setErrorTopicData(topicName, e.toString());
    } finally {
      // æ¸…é™¤åŠ è½½çŠ¶æ€
      _isLoadingTopicDetails = false;
      _loadingTopic = null;
      notifyListeners();
    }
  }

  /// è®¾ç½®æœ€å°åŒ–ä¸»é¢˜æ•°æ®ï¼ˆå½“è¿æ¥ä¸å¯ç”¨æ—¶ï¼‰
  void _setMinimalTopicData(String topicName) {
    _topicDetails[topicName] = TopicInfo(
      name: topicName,
      partitions: 0, // æ˜ç¡®æ˜¾ç¤ºæ— åˆ†åŒºä¿¡æ¯
      replicationFactor: 0,
      latestOffset: 0, // ä¸å†ä½¿ç”¨å›ºå®šå€¼
      earliestOffset: 0,
      inSyncReplicas: 0,
      offlineReplicas: 0,
      createdTime: 'N/A',
      lastModifiedTime: 'N/A',
      isInternal: topicName.startsWith('__'),
    );

    _topicPartitions[topicName] = [];
    _topicConfigs[topicName] = [
      KafkaConfigParam(name: 'status', value: 'disconnected', isDefault: true, isReadOnly: true),
    ];
    _topicConsumerGroups[topicName] = [];
  }

  /// è®¾ç½®é”™è¯¯çŠ¶æ€æ•°æ®
  void _setErrorTopicData(String topicName, String error) {
    _topicDetails[topicName] = TopicInfo(
      name: topicName,
      partitions: -1, // è¡¨ç¤ºé”™è¯¯çŠ¶æ€
      replicationFactor: -1,
      latestOffset: -1,
      earliestOffset: -1,
      inSyncReplicas: -1,
      offlineReplicas: -1,
      createdTime: 'Error',
      lastModifiedTime: 'Error',
      isInternal: topicName.startsWith('__'),
    );

    _topicPartitions[topicName] = [];
    _topicConfigs[topicName] = [
      KafkaConfigParam(name: 'error', value: error, isDefault: false, isReadOnly: true),
    ];
    _topicConsumerGroups[topicName] = [];
  }
}
